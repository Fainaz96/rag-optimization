{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cbd69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U openai supabase python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "280235cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import date\n",
    "from typing import List, Dict, Any\n",
    "from openai import OpenAI\n",
    "from supabase import create_client\n",
    "\n",
    "# ==== CONFIG ====\n",
    "OPENAI_CHAT_MODEL = os.getenv(\"OPENAI_CHAT_MODEL\", \"gpt-4o-mini\")\n",
    "OPENAI_EMBEDDING_MODEL = os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")  # 1536-dim\n",
    "SUPABASE_URL = os.environ[\"SUPABASE_URL\"]                    # set in your environment\n",
    "SUPABASE_KEY = os.environ[\"SUPABASE_KEY\"]       # or ANON if your RLS allows it\n",
    "MATCH_RPC = os.getenv(\"RAG_MATCH_RPC\", \"match_documents\")    # your RPC name\n",
    "DEFAULT_FILTER = {\"client_id\": \"ccs\"}                        # mirrors your N8N filter\n",
    "TOP_K = int(os.getenv(\"RAG_TOP_K\", \"6\"))\n",
    "FALLBACK = \"I’m sorry, I couldn’t find relevant information based on your documents.\"\n",
    "\n",
    "SYSTEM_PROMPT = f\"\"\"Today is: {date.today().isoformat()}\n",
    "You are a Retrieval-Augmented Generation (RAG) assistant for CCS. Answer using ONLY the retrieved documents.\n",
    "If the answer is not in the documents, reply exactly with: \"{FALLBACK}\".\n",
    "Be concise, neutral, and factual.\n",
    "\"\"\"\n",
    "\n",
    "# ==== CLIENTS ====\n",
    "oai = OpenAI()  # uses OPENAI_API_KEY\n",
    "sb = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "# ==== RAG CORE ====\n",
    "def _embed(text: str) -> List[float]:\n",
    "    return oai.embeddings.create(model=OPENAI_EMBEDDING_MODEL, input=text).data[0].embedding\n",
    "\n",
    "def retrieve(query: str, k: int = TOP_K, flt: Dict[str, Any] = DEFAULT_FILTER) -> List[Dict[str, Any]]:\n",
    "    emb = _embed(query)\n",
    "    res = sb.rpc(MATCH_RPC, {\"query_embedding\": emb, \"match_count\": k, \"filter\": flt}).execute()\n",
    "    return res.data or []\n",
    "\n",
    "def _format_context(rows: List[Dict[str, Any]]) -> str:\n",
    "    parts = []\n",
    "    for i, r in enumerate(rows, 1):\n",
    "        meta = r.get(\"metadata\") or {}\n",
    "        src = meta.get(\"source\") or meta.get(\"path\") or meta.get(\"url\") or f\"doc_{i}\"\n",
    "        txt = r.get(\"content\") or r.get(\"page_content\") or r.get(\"text\") or \"\"\n",
    "        parts.append(f\"[{i}] {src}\\n{txt}\")\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "def answer(question: str, k: int = TOP_K, flt: Dict[str, Any] = DEFAULT_FILTER) -> Dict[str, str]:\n",
    "    rows = retrieve(question, k=k, flt=flt)\n",
    "    if not rows:\n",
    "        return {\"answer\": FALLBACK, \"sources\": \"\"}\n",
    "\n",
    "    context = _format_context(rows)\n",
    "    msgs = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": f'Use ONLY the context below. If it’s insufficient, reply exactly with \"{FALLBACK}\".\\n\\n{context}'}\n",
    "    ]\n",
    "    resp = oai.chat.completions.create(model=OPENAI_CHAT_MODEL, temperature=0, messages=msgs)\n",
    "    out = resp.choices[0].message.content\n",
    "\n",
    "    src_list = []\n",
    "    for i, r in enumerate(rows, 1):\n",
    "        meta = r.get(\"metadata\") or {}\n",
    "        src = meta.get(\"source\") or meta.get(\"path\") or meta.get(\"url\") or f\"doc_{i}\"\n",
    "        src_list.append(f\"[{i}] {src}\")\n",
    "\n",
    "    return {\"answer\": out, \"sources\": \"\\n\".join(src_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "937c66d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The upcoming CCS events are as follows:\n",
      "\n",
      "1. **CHRS Annual Meeting 2025**\n",
      "   - Date: September 12-13, 2025\n",
      "   - Location: Halifax, NS\n",
      "\n",
      "2. **Pediatric Cardiology Trainee Review Program – Session 1**\n",
      "   - Date: October 3, 2025\n",
      "   - Location: Virtual via Zoom\n",
      "\n",
      "3. **Pediatric Cardiology Trainee Review Program – Session 2**\n",
      "   - Date: October 22, 2025\n",
      "   - Location: In-person at the Canadian Cardiovascular Congress, Quebec City\n",
      "\n",
      "4. **Canadian Cardiovascular Congress 2025**\n",
      "   - Date: October 23-26, 2025\n",
      "   - Location: Quebec City, QC\n",
      "\n",
      "5. **Adult Cardiology Trainee Review Program**\n",
      "   - Date: November 28-30, 2025\n",
      "   - Location: TBD\n",
      "\n",
      "SOURCES\n",
      "-------\n",
      "[1] https://ccs.ca/news/2023-call-for-ccs-guideline-and-clinical-practice-update-topics-now-live/\n",
      "[2] https://ccs.ca/news/2023-call-for-ccs-guideline-and-clinical-practice-update-topics-now-live/\n",
      "[3] https://ccs.ca/latest-events/\n",
      "[4] https://ccs.ca/latest-events/\n",
      "[5] https://ccs.ca/latest-events/\n",
      "[6] https://ccs.ca/latest-events/\n"
     ]
    }
   ],
   "source": [
    "res = answer(\"What are the ccs upcoming events?\")\n",
    "print(res[\"answer\"])\n",
    "print(\"\\nSOURCES\\n-------\\n\" + (res[\"sources\"] or \"—\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
